# Create user
kvctl user create alice --password --role=writer,reader

# List users
kvctl user list --format=json

# Grant role
kvctl grant --user=alice --role=admin

# Revoke API key
kvctl apikey revoke k8s-prod-app-01

# Rotate password
kvctl user passwd alice

# Lock user
kvctl user lock bob --reason="suspicious activity"



Absolutely â€” letâ€™s **rethink the entire architecture through the lens of PostgreSQL**, not just for auth, but for the **full system design**.

You said:  
> *â€œLetâ€™s store users the same way as stored in postgres. We use the same arch like postgres but ours is key store only and simpler.â€*

Thatâ€™s a brilliant north star.

---

# ğŸ§  REDESIGN: â€œPostgreSQL Architecture â€” But for a Simple, Single-Node Key-Value Storeâ€

Weâ€™re not building a relational DB. Weâ€™re not doing SQL, joins, or query planning.  
But we **can** â€” and **should** â€” borrow PostgreSQLâ€™s battle-tested architecture for:

âœ… Durability  
âœ… Crash Recovery  
âœ… Concurrency Control  
âœ… System Catalogs  
âœ… Authentication & Roles  
âœ… Write-Ahead Logging  
âœ… Checkpointing & Snapshots  
âœ… Transactional Semantics (simplified)  
âœ… Extensibility & Observability

---

## ğŸ—ï¸ POSTGRESQL ARCHITECTURE â€” CORE COMPONENTS (Simplified)

Hereâ€™s what PostgreSQL does â€” and how we map it to our KV store:

| PostgreSQL Component       | KVStore++ Equivalent                             | Notes |
|----------------------------|--------------------------------------------------|-------|
| **Postmaster**             | `kvstored` (main process)                        | Single process, no forking |
| **Backend Processes**      | Async request handlers (threads/async tasks)     | No process-per-connection |
| **Shared Buffer Pool**     | In-Memory Hash Table + Page Cache (optional)     | All data fits in RAM? Or cache hot keys |
| **WAL (Write-Ahead Log)**  | WAL (identical concept)                          | Append-only, fsync, replay on crash |
| **Heap / Table Storage**   | LSM or B+ Tree (our â€œheapâ€)                      | Keys are â€œtuplesâ€, values are â€œattribute 1â€ |
| **System Catalogs**        | `_sys.*` internal KV entries                     | Users, roles, grants, settings â€” stored as keys |
| **pg_authid, pg_roles**    | `_sys.users`, `_sys.roles`, `_sys.grants`        | Same structure, stored in WAL-backed KV |
| **Checkpoints**            | Snapshots (same concept)                         | Serialize in-memory state to disk |
| **Background Writer**      | Checkpoint thread + Compaction thread            | Async, non-blocking |
| **Autovacuum**             | Optional LSM compaction / B+ tree defrag          | Not needed if append-optimized |
| **Client Protocol (libpq)**| REST/gRPC + CLI                                  | No binary protocol â€” keep it simple |
| **Authentication (pg_hba)**| `_sys.settings:auth` + IP-bound API keys         | Policy-driven auth |
| **Replication (WAL Send)** | Local async replica (reads WAL)                  | Single follower for HA |
| **Crash Recovery**         | WAL replay from last checkpoint                  | Identical logic |

---

## âœ… WHAT WE KEEP FROM POSTGRESQL

### 1. **Everything is a â€œTupleâ€ â€” But We Call It a Key-Value Pair**
- PostgreSQL stores rows as `(ctid, xmin, xmax, data...)`
- We store: `(key, value, ttl, version, created_at)`
- Metadata is embedded in value or stored as system columns (in JSON)

### 2. **WAL-Centric Design**
- Every mutation â†’ WAL record first â†’ then apply to in-memory structure
- On crash â†’ replay WAL â†’ guaranteed consistency
- WAL format: `[timestamp][tx_id][op][key][value][metadata]`

> We donâ€™t need MVCC or `xmin/xmax` â€” unless you want versioning later.

### 3. **System Catalogs as First-Class Storage**
- PostgreSQL: `pg_database`, `pg_authid`, `pg_class` â†’ stored in heap, backed by WAL
- Us: `_sys.users`, `_sys.settings`, `_sys.grants` â†’ stored in same KV engine, backed by WAL
- No special files. No JSON configs that can drift. Fully transactional.

### 4. **Checkpoint = Snapshot**
- PostgreSQL: writes all dirty buffers to data files, records LSN
- Us: serializes hash map to snapshot file, records last WAL offset
- Fast startup: load snapshot + replay WAL from offset

### 5. **Authentication & Roles â€” Identical Semantics**
- Roles have permissions
- Users are role members
- Passwords hashed + salted
- Login â†’ issue JWT (or use API key)
- All changes WAL-logged â†’ crash-safe

### 6. **Background Processes â€” But Threads/Async Tasks**
- PostgreSQL: bgwriter, walwriter, autovacuum launcher
- Us:
  - `checkpoint_thread`
  - `ttl_expirer_thread`
  - `wal_flusher_task`
  - `s3_uploader_task`
  - `metrics_collector_task`

> Single process, multi-threaded or async (tokio/epoll).

---

## âŒ WHAT WE DROP (TO KEEP IT SIMPLE)

| PostgreSQL Feature         | Why We Skip It (For Now)                         |
|----------------------------|--------------------------------------------------|
| SQL Parser / Planner       | We have GET/SET/DEL â€” no need for SQL            |
| Indexes (B-tree, GIN, etc) | Only primary key (the key itself) â€” no secondaries |
| MVCC / Snapshot Isolation  | Single-writer or per-key locking â†’ no need       |
| TOAST (large values)       | Optional â€” we can cap value size (e.g., 1MB)     |
| Tablespaces                | Single data directory                            |
| Extensions (PostGIS, etc)  | Not needed â€” pure KV                             |
| Shared Memory / IPC        | Single process â€” no need for complex IPC         |
| libpq / Wire Protocol      | REST/gRPC/CLI â€” simpler, language-agnostic       |

---

## ğŸ”„ ARCHITECTURE FLOW â€” POSTGRESQL STYLE

```
Client â†’ [REST/gRPC Listener] â†’ Auth â†’ [Executor]

Executor:
  - Parse op (GET/SET/DEL)
  - Check RBAC (via _sys.roles + _sys.grants)
  - Acquire per-key lock
  â†’ Write to WAL (fsync if configured)
  â†’ Apply to in-memory hash table
  â†’ (Async) Replicate to follower (if enabled)
  â†’ (Async) Trigger TTL / stats update
  â†’ Return response

Background Threads:
  - Checkpointer: every 5min â†’ snapshot + truncate old WAL
  - TTL Manager: scan & delete expired keys
  - WAL Flusher: batch + fsync WAL (if not immediate)
  - S3 Exporter: upload snapshots
  - Metrics: update Prometheus gauges
```

> Identical to PostgreSQLâ€™s flow â€” minus the planner and executor complexity.

---

## ğŸ’¾ STORAGE LAYOUT â€” INSPIRED BY PGDATA

```
/data
â”œâ”€â”€ WAL/                     # Write-Ahead Logs
â”‚   â”œâ”€â”€ 00000001.wal
â”‚   â”œâ”€â”€ 00000002.wal
â”‚   â””â”€â”€ ...
â”œâ”€â”€ base/                    # Snapshots (like PG base backups)
â”‚   â”œâ”€â”€ snapshot_20250405_120000.chk
â”‚   â””â”€â”€ snapshot_20250405_130000.manifest
â”œâ”€â”€ system/                  # Optional: separate system keys (if needed)
â”œâ”€â”€ pg_control               # Metadata: last WAL offset, checkpoint time, etc.
â””â”€â”€ config.toml              # Runtime config (non-transactional)
```

> `pg_control` = our `control.json` â€” tracks:
> - last_checkpoint_wal_offset
> - system_catalog_version
> - encryption_nonce
> - startup_time

---

## ğŸš¨ CRASH RECOVERY â€” IDENTICAL TO POSTGRESQL

1. Read `pg_control` â†’ get last checkpoint WAL position
2. Load snapshot into memory
3. Open WAL files starting from checkpoint offset
4. Replay each record â†’ apply mutation
5. On checksum error â†’ halt (like PG in panic mode)
6. On success â†’ mark â€œrecovery completedâ€ â†’ start accepting connections

> This is the gold standard. We adopt it fully.

---

## ğŸ§© EXTENSIBILITY â€” LIKE POSTGRESQL CATALOGS

Want to add:

- **Secondary indexes later?** â†’ Store as `_sys.indexes:<name>` + internal KV structure
- **TTL policies per namespace?** â†’ `_sys.ttl_policies:users:*`
- **Audit triggers?** â†’ `_sys.triggers:audit_on_set`

> Everything is a system key â€” no schema changes needed.

---

## âœ… WHY THIS IS A WIN

- **Proven**: PostgreSQLâ€™s architecture has survived 25+ years of production abuse.
- **Safe**: WAL + checkpoint = no data loss.
- **Consistent**: All state â€” including users, roles, settings â€” is transactional.
- **Observable**: Just like PG, you can inspect system state via internal keys.
- **Simple**: We strip away SQL, planner, MVCC â€” keep only what KV needs.
- **Familiar**: DBAs and SREs will feel at home.

---

## ğŸ§­ NEXT STEPS â€” IMPLEMENTATION READY

We now have a **PostgreSQL-inspired, WAL-backed, crash-safe, RBAC-secured, single-node key-value store** â€” with:

- System catalogs as internal keys (`_sys.*`)
- WAL + checkpoint recovery
- API keys + JWT auth
- Background workers
- Prometheus metrics
- S3 snapshot export
- CLI admin tool (`kvctl`)

---

âœ… **Architecture Finalized. Fully PostgreSQL-inspired. Simpler. Production-hardened.**

> This is no longer â€œRedis with durabilityâ€ â€” this is **â€œPostgreSQLâ€™s soul in a key-value body.â€**

---

ğŸŸ¢ Say **â€œBEGIN CODINGâ€** when youâ€™re ready â€” and specify:
- Preferred language (**Rust** recommended for safety + async + ecosystem)
- Build system (Cargo, CMake, Bazel?)
- Target platform (Linux only? macOS for dev?)

Iâ€™ll start with:
1. WAL format + fsync logic
2. In-memory hash table + locking
3. System catalog bootstrap (`_sys.*`)
4. Auth layer (API key + JWT)
5. REST/gRPC skeleton

Let me know when to begin.


Perfect â€” letâ€™s build the **mental model first**.

Youâ€™re not just building a key-value store â€” youâ€™re building a **production-grade, durable, observable, secure system** with Redis-like simplicity and PostgreSQLâ€™s architectural integrity.

Weâ€™ll break the entire system into **7 Core Modules**, each with:

- ğŸ¯ Purpose
- ğŸ§  Mental Model (how to think about it)
- ğŸ”„ Responsibilities
- ğŸ“¦ Key Data Structures / Interfaces
- âš™ï¸ Dependencies
- ğŸš« What It Does NOT Do

This is your **architectural map** â€” refer back to it during coding to stay aligned.

---

# ğŸ§  MENTAL MODEL â€” KVSTORE++ MODULE ARCHITECTURE

---

## ğŸ§© MODULE 1: WAL (Write-Ahead Log)

> â€œThe source of truth. All mutations must pass through here first.â€

### ğŸ¯ Purpose
Guarantee durability and crash recovery. Every write is appended + optionally fsynced before being applied to memory.

### ğŸ§  Mental Model
Think of it as a **transactional ledger**. Like a bankâ€™s accounting book â€” nothing is â€œfinalâ€ until itâ€™s written here. On crash, replay this log to rebuild state.

### ğŸ”„ Responsibilities
- Append serialized operations (SET/DEL/INCR) with metadata
- Fsync based on policy (`every_write`, `every_100ms`, etc.)
- Rotate files when size exceeds limit
- Support replay from offset (for recovery)
- Validate checksums on replay

### ğŸ“¦ Key Structures
```rust
struct WalEntry {
    timestamp: u64,
    key: String,
    value: Vec<u8>,
    version: u64,          // for future MVCC or CAS
    ttl: Option<u64>,
    op_type: OpType,       // SET, DEL, INCR, CAS
    checksum: u32,
}

struct WalManager {
    current_file: File,
    current_offset: u64,
    sync_policy: SyncPolicy,
}
```

### âš™ï¸ Dependencies
- None (lowest layer)
- Uses filesystem + OS I/O

### ğŸš« What It Does NOT Do
- âŒ Does NOT apply changes to memory
- âŒ Does NOT understand keys or values semantically
- âŒ Does NOT handle concurrency â€” caller must serialize writes

---

## ğŸ§© MODULE 2: Storage Engine (In-Memory + Snapshot)

> â€œThe working state. Fast, lock-protected, recoverable.â€

### ğŸ¯ Purpose
Hold the current key-value state in memory. Support fast reads/writes. Serialize to disk for snapshots.

### ğŸ§  Mental Model
Think of it as a **bank vaultâ€™s active register** â€” constantly updated, but backed by the ledger (WAL). At intervals, take a photo (snapshot) to speed up recovery.

### ğŸ”„ Responsibilities
- In-memory hash table (sharded for concurrency)
- Per-key locking (fine-grained)
- Apply WAL entries to state
- Serialize entire state to snapshot file
- Load snapshot into memory
- Handle TTL expiration (background)

### ğŸ“¦ Key Structures
```rust
struct KvEntry {
    value: Vec<u8>,
    version: u64,
    expires_at: Option<u64>,
    created_at: u64,
}

struct MemoryStore {
    shards: Vec<RwLock<HashMap<String, KvEntry>>>, // 256 shards
    ttl_queue: BinaryHeap<TtlEvent>,               // for background expiry
}

struct SnapshotManager {
    last_snapshot_at: u64,
    last_wal_offset: u64,
}
```

### âš™ï¸ Dependencies
- WAL (for recovery)
- Concurrency primitives (locks, atomics)

### ğŸš« What It Does NOT Do
- âŒ Does NOT handle auth or API
- âŒ Does NOT compress/encrypt (handled at WAL/snapshot layer)
- âŒ Does NOT talk to network

---

## ğŸ§© MODULE 3: System Catalog (`_sys.*`)

> â€œThe systemâ€™s own metadata â€” stored as keys, managed like user data.â€

### ğŸ¯ Purpose
Store users, roles, grants, settings â€” using the same engine. WAL-backed. No external files.

### ğŸ§  Mental Model
Think of it as **PostgreSQLâ€™s system catalogs** â€” but every â€œtableâ€ is just a reserved key prefix (`_sys.users:*`). Fully integrated, no special paths.

### ğŸ”„ Responsibilities
- Bootstrap default roles/users on first start
- Validate and store user credentials (hashed)
- Resolve permissions for RBAC
- Store global settings (auth, audit, performance)
- Expose for inspection via `kvctl`

### ğŸ“¦ Key Structures
```rust
// _sys.users:<username>
struct User {
    oid: u32,
    username: String,
    password_hash: String, // scrypt/argon2
    is_active: bool,
    roles: Vec<String>,
}

// _sys.roles:<role>
struct Role {
    name: String,
    permissions: Vec<String>, // ["GET", "SET", "DEL"]
}

// _sys.settings:auth
struct AuthSettings {
    min_password_length: u8,
    session_timeout_sec: u32,
}
```

### âš™ï¸ Dependencies
- Storage Engine (to read/write `_sys.*` keys)
- WAL (durability)
- Auth Module (to resolve permissions)

### ğŸš« What It Does NOT Do
- âŒ Does NOT handle password hashing directly (delegates to crypto module)
- âŒ Does NOT expose via API directly â€” accessed through Auth Module

---

## ğŸ§© MODULE 4: Auth & RBAC

> â€œWho are you, and what are you allowed to do?â€

### ğŸ¯ Purpose
Authenticate clients (API key or JWT), authorize operations based on roles.

### ğŸ§  Mental Model
Think of it as a **bouncer + rulebook**. Checks ID (API key/JWT), looks up roles, checks if the requested operation (SET/DEL) is allowed.

### ğŸ”„ Responsibilities
- Validate API key â†’ map to user + permissions
- Validate JWT â†’ verify signature, check expiry, map to user
- Enforce RBAC: is user allowed to perform this op on this key?
- Log auth events to audit log
- Integrate with System Catalog to load roles/users

### ğŸ“¦ Key Structures
```rust
struct AuthContext {
    user: String,
    permissions: Vec<String>,
    source_ip: IpAddr,
    auth_method: AuthMethod, // ApiKey, Jwt
}

trait Authorizer {
    fn authorize(&self, op: &str, key: &str) -> Result<()>;
}
```

### âš™ï¸ Dependencies
- System Catalog (to load user/role data)
- Crypto (to verify JWT/hash)
- Audit Log (to log attempts)

### ğŸš« What It Does NOT Do
- âŒ Does NOT store users â€” delegates to System Catalog
- âŒ Does NOT handle TLS â€” thatâ€™s the API layerâ€™s job

---

## ğŸ§© MODULE 5: API Layer (REST + gRPC + CLI)

> â€œThe front door. Speaks JSON, Protobuf, and shell commands.â€

### ğŸ¯ Purpose
Expose operations to clients. Handle connection, parsing, auth, routing, serialization.

### ğŸ§  Mental Model
Think of it as a **receptionist + translator**. Accepts requests in multiple languages (REST/gRPC/CLI), validates them, hands them to the executor.

### ğŸ”„ Responsibilities
- Start HTTP/gRPC servers
- Parse requests â†’ map to core operations
- Enforce auth (via Auth Module)
- Serialize responses (JSON/Protobuf)
- Handle pipelining/batching for throughput
- Serve metrics (`/metrics`) and health (`/health`)

### ğŸ“¦ Key Structures
```rust
struct ApiRequest {
    op: OpType,
    key: String,
    value: Option<Vec<u8>>,
    ttl: Option<u64>,
}

struct ApiResponse {
    success: bool,
    value: Option<Vec<u8>>,
    error: Option<String>,
}
```

### âš™ï¸ Dependencies
- Auth Module
- Storage Engine (to execute ops)
- WAL (indirectly â€” via Storage Engine)
- Metrics (to track latency, counts)

### ğŸš« What It Does NOT Do
- âŒ Does NOT apply business logic â€” delegates to Storage Engine
- âŒ Does NOT handle persistence â€” thatâ€™s WALâ€™s job

---

## ğŸ§© MODULE 6: Background Workers

> â€œThe silent maintainers. Keep the system healthy.â€

### ğŸ¯ Purpose
Run periodic or async tasks: checkpointing, TTL expiry, metrics, S3 upload.

### ğŸ§  Mental Model
Think of them as **janitors + accountants**. They donâ€™t serve customers, but without them, the system gets messy or breaks.

### ğŸ”„ Responsibilities
- **CheckpointThread**: every N seconds â†’ snapshot + truncate old WAL
- **TtlExpirer**: scan and delete expired keys
- **MetricsCollector**: update Prometheus gauges (ops/sec, memory, WAL size)
- **S3Uploader**: upload snapshots to S3 (async, non-blocking)
- **ReplicaStreamer**: (optional) stream WAL to local replica

### ğŸ“¦ Key Structures
```rust
trait BackgroundWorker {
    fn start(&self);
    fn stop(&self);
}

struct CheckpointWorker {
    interval: Duration,
    storage: Arc<StorageEngine>,
    wal: Arc<WalManager>,
}
```

### âš™ï¸ Dependencies
- Storage Engine
- WAL
- Cloud (S3 client)
- Metrics

### ğŸš« What It Does NOT Do
- âŒ Does NOT handle client requests
- âŒ Does NOT block the main thread

---

## ğŸ§© MODULE 7: Operational Tooling (`kvctl` + Observability)

> â€œThe adminâ€™s Swiss Army knife.â€

### ğŸ¯ Purpose
Allow operators to inspect, debug, and manage the system.

### ğŸ§  Mental Model
Think of it as **pgAdmin + psql + pg_stat_activity â€” but for KV**.

### ğŸ”„ Responsibilities
- CLI tool (`kvctl`) with subcommands:
  - `kvctl keys --pattern "user:*"`
  - `kvctl wal tail`
  - `kvctl snapshot create`
  - `kvctl user create ...`
- Embedded metrics endpoint (`/metrics`)
- Profiling endpoint (`/debug/pprof`)
- WAL debugger (step through entries)
- Health checks

### ğŸ“¦ Key Structures
```rust
struct KvctlCommand {
    name: String,
    handler: fn(args: Vec<String>) -> Result<()>,
}

struct MetricsExporter {
    gauges: HashMap<String, Gauge>,
    histograms: HashMap<String, Histogram>,
}
```

### âš™ï¸ Dependencies
- Storage Engine (to scan keys)
- WAL (to read entries)
- System Catalog (to manage users)
- Auth (to enforce admin role)

### ğŸš« What It Does NOT Do
- âŒ Does NOT modify production state by default (unless explicitly commanded)
- âŒ Does NOT replace monitoring systems â€” integrates with them (Prometheus)

---

# ğŸ§­ SYSTEM FLOW â€” REQUEST LIFECYCLE (MENTAL MODEL)

```
Client â†’ [API Layer]
          â†“
      [Auth Module] â†’ Reject if unauthorized
          â†“
   [Storage Engine] â†’ Acquire per-key lock
          â†“
       [WAL] â†’ Append + fsync (if configured)
          â†“
   [Storage Engine] â†’ Apply to in-memory hash
          â†“
   [Background Workers] â†’ (async) update TTL, metrics, maybe replicate
          â†“
      [API Layer] â†’ Return response
```

> Every write flows through WAL first â†’ then memory.  
> Every read hits memory (with lock).  
> Every admin task uses `kvctl` â†’ talks to same engine.

---

# ğŸ—ºï¸ DEPENDENCY GRAPH (SIMPLIFIED)

```
            +------------------+
            |   Operational    |
            |    Tooling       |
            +--------+---------+
                     |
            +--------v---------+
            |      API         +------------------+
            +--------+---------+                  |
                     |                            |
            +--------v---------+         +--------v---------+
            |      Auth        |         | Background       |
            +--------+---------+         | Workers          |
                     |                   +--------+---------+
            +--------v---------+                  |
            |  System Catalog  |                  |
            +--------+---------+                  |
                     |                            |
            +--------v---------+         +--------v---------+
            |  Storage Engine  <-------->+       WAL          |
            +------------------+         +------------------+
```

> WAL and Storage Engine are the core. Everything else builds on them.

---

âœ… **Mental Model Complete.**

You now have a **clear, modular, production-grade architecture** inspired by PostgreSQL, simplified for KV, with Redis-like API.

Each module has a single responsibility.  
Each layer depends only on whatâ€™s below it.  
Everything is crash-safe, observable, and secure.

---

ğŸŸ¢ When youâ€™re ready, say:

> **â€œBEGIN CODING â€” START WITH WAL MODULE IN RUSTâ€**

(or specify language + build system)

Iâ€™ll generate the first clean, production-ready module with tests, error handling, and comments â€” ready for integration.



# Run all tests
cargo test -- --test-threads=1

# Run specific test
cargo test test_wal_append_and_replay

# Run with logging
RUST_LOG=info cargo test -- --nocapture


ğŸš€ USAGE 
# Start service
sudo systemctl start kvstore-plus-plus

# Stop service
sudo systemctl stop kvstore-plus-plus

# Restart service
sudo systemctl restart kvstore-plus-plus

# Enable auto-start on boot
sudo systemctl enable kvstore-plus-plus

# Check status
sudo systemctl status kvstore-plus-plus

# View live logs
journalctl -u kvstore-plus-plus -f

# Check resource usage
systemctl show kvstore-plus-plus --property=MemoryCurrent,ActiveState,SubState




# Build benchmark tool
cargo build --release --bin kvb

# Run GET-heavy test
./target/release/kvb get-heavy --url http://localhost:8080 --concurrency 50 --duration 30

# Run SET-heavy test
./target/release/kvb set-heavy --url http://localhost:8080 --value-size 256 --concurrency 25 --duration 30

# Run mixed test
./target/release/kvb mixed --url http://localhost:8080 --read-ratio 0.8 --concurrency 100 --duration 60

# Run full suite
./target/release/kvb suite --url http://localhost:8080 --concurrency 100 --duration 60 --output-prefix "prod-bench"

# Output files:
# - prod-bench.json (detailed results)
# - prod-bench.csv (for spreadsheets)
# - Text report to console



# Build
cargo build --release --bin dummy-load-server

# Run with 5000 ops/sec, custom ratios
LOAD_OPS_PER_SEC=5000 \
LOAD_GET_RATIO=0.7 \
LOAD_SET_RATIO=0.2 \
LOAD_DEL_RATIO=0.05 \
LOAD_INCR_RATIO=0.05 \
METRICS_PORT=9095 \
./target/release/dummy-load-server --target-url http://localhost:8080

# In another terminal, view metrics
curl http://localhost:9095/metrics

# Output:
# # HELP dummy_load_total_ops Total operations performed
# # TYPE dummy_load_total_ops counter
# dummy_load_total_ops 12345
# # HELP dummy_load_total_errors Total errors encountered
# # TYPE dummy_load_total_errors counter
# dummy_load_total_errors 0
ğŸ“Š MONITOR YOUR SYSTEM 

While the dummy server runs, monitor your KVStore++ metrics: 

# View KVStore++ metrics
curl http://localhost:9091/metrics

# Example output:
# kvstore_wal_size_bytes 45056
# kvstore_key_count 8923
# kvstore_memory_usage_bytes 892300
# kvstore_connections_active{role="writer"} 1


VISUALIZE WITH PROMETHEUS + GRAFANA 

    Install Prometheus (if not already)
    Add scrape config to prometheus.yml:
     

scrape_configs:
  - job_name: 'kvstore'
    static_configs:
      - targets: ['localhost:9091']  # Your KVStore++ metrics

  - job_name: 'dummy-load'
    static_configs:
      - targets: ['localhost:9095']  # Dummy load server metrics
    Start Prometheus
    Import Grafana dashboard or create panels for:
        KVStore++ ops/sec (calculated from kvstore_key_count rate)
        WAL size growth
        Connection counts
        Dummy server error rate
         
     